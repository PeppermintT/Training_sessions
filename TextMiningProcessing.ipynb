{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UK Data Service -  Text Mining Basic Processes Tutorial\n",
    "\n",
    "I am working through the code files at my own pace to grasp the processing steps and reflect on how I can apply to my own data.\n",
    "https://github.com/UKDataServiceOpen/text-mining/blob/master/code/tm-processing-2020-06-16.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key learning points:\n",
    "   * tokenisation, standardisation, removing irregularlies and consolidation (i.e lemmatisation or stemming) are key stages of the processing. However the order will vary depending on the research question and the text.\n",
    "   * Really important to keep track of variables, particularly if you are experimenting with different token sizes.\n",
    "   * Need to get better at RegEx expressions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Succesfully imported necessary modules\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import the modules and packages \n",
    "\n",
    "import os   # os is a module for navigating your machine (e.g., file directories).\n",
    "import nltk # nltk stands for natural language tool kit and is useful for text-mining. \n",
    "import re  #  re is for regular expressions, which we use later \n",
    "\n",
    "print(\"1. Succesfully imported necessary modules\")    # The print statement is just a bit of encouragement!\n",
    "\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the \"sample_text\" file and read (import) its contents to a variable called \"corpus\"\n",
    "with open(\"C:/Users/sonja/Desktop/TfL/Furlough Learning/sample_text.txt\", \"r\", encoding = \"ISO-8859-1\") as f:\n",
    "    corpus = f.read()\n",
    "    \n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hmm this all looks a bit dull. Nevermind; it's an example.\n",
    "#Let's see what datatype this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus)# it's one long string...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a few steps of processing to do:\n",
    "\n",
    "* Tokenisation, (or splitting text into various kinds of 'short things' that can be statistically analysed).\n",
    "* Standardising the next (including converting uppercase to lower, correcting spelling, find-and-replace operations to remove abbreviations, etc.).\n",
    "* Removing irrelevancies (anything from punctuation to stopwords like 'the' or 'to' that are unhelpful for many kinds of analysis).\n",
    "* Consolidating (including stemming and lemmatisation that strip words back to their 'root').\n",
    "* Basic NLP (that put some of the small things back together into logically useful medium things, like multi-word noun or verb phrases and proper names).\n",
    "\n",
    "In practice, most text-mining work will require that any given corpus undergo multiple steps, but the exact steps and the exact order of steps depends on the desired analysis to be done. \n",
    "\n",
    "Also - it is important to create new variables at the different stages of the process so that it is possible to return to previous stages easily.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to cut our 'one big thing' into tokens, or 'lots of little things'. For most text-mining work will also want to divide the corpus into what are known as 'tokens'. These 'tokens' are the unit of analysis, which might be chapters, sections, paragraphs, sentences, words, or something else.\n",
    "\n",
    "As this corpus is already in the form of a string, we can tokenize both into words and sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "['This', 'is', 'a', 'sample', 'corpus', '.', 'It', 'haz', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', '.', 'For', 'example', ',', 'it', 'has', 'both', '1972', 'and', 'ninety-six', '.', 'This', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', ',', 'but', 'not', 'always', '.', 'California', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'CA', '.', 'To', 'really', 'complicate', 'things', ',', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'U.K.', ',', 'the', 'UK', ',', 'the', 'United', 'Kingdom', ',', 'the', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'The', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'Northern', 'Ireland', 'becuase', 'sometimes', 'full', 'names', 'are', 'important', '.', 'Further', ',', 'here', 'is', 'a', 'bunch']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "from nltk import word_tokenize  # importing the word_tokenize function from nltk\n",
    "corpus_words = word_tokenize(corpus) # Pass the corpus through word tokenize \n",
    "print(corpus_words[:100])    # the [:10] within the print statement says \n",
    "type(corpus_words)      # Always good to know your variable type!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Punctuation is one token\n",
    " - Interestingly both UK and U.K. are one token\n",
    "\n",
    "Word_tokenize is a useful function if you want to take a 'bag of words' approach to text-mining. This ignores how the words were used or in what order they originally appeared, making it easy to count how often each word occurrs. You can glean a lot of  info from this. Important to note that this does not distinguish between verbs and nouns always - eg \"building\" the verb will be treated in the same way as the \"building\" (noun). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a sample corpus.', 'It haz some spelling errors and has numbers written two ways.', 'For example, it has both 1972 and ninety-six.', 'This sample corpus also uses abbreviations sometimes, but not always.', 'California is spelled out once but also written CA.', 'To really complicate things, another country name is written as the U.K., the UK, the United Kingdom, the United Kingdom of Great Britain and The United Kingdom of Great Britain and Northern Ireland becuase sometimes full names are important.', 'Further, here is a bunch of unrelated toxt just to fill up the space.', 'This privacy policy (â\\x80\\x9cPrivacy Policyâ\\x80\\x9d) is intended to inform you of some policies and practices regarding the collection, use, and disclosure of your Personal Information through our site and any other sites that links to this Privacy Policy (the â\\x80\\x9cSiteâ\\x80\\x9d).', 'We define â\\x80\\x9cPersonal Informationâ\\x80\\x9d as information that allows someone to identify you personally or contact you, including for example your name, address, telephone number, and email address.', 'By registering with us or using our Site, you expressly consent to the collection, use, processing, and disclosure of your Personal Information in accordance with this Privacy Policy.']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing sent_tokenize from nltk\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "# Same again, but this time broken into sentences\n",
    "corpus_sentences = sent_tokenize(corpus)\n",
    "print(corpus_sentences[:10])        # Since these are sentences instead of words \n",
    "#we'll, take the first 10\n",
    "\n",
    "#check type\n",
    "type(corpus_sentences)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Corpus_sentences is also a list of strings - they stand and end with square brackets and each item has single quotes. Here full stops at the end of each sentence are included within the sentence token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a \"bag of words\" approach we don't need to distinguish between lower case and capitals. Creating a new variable, corpus_lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'corpus', '.', 'it', 'haz', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', '.', 'for', 'example', ',', 'it', 'has', 'both', '1972', 'and', 'ninety-six', '.', 'this', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', ',', 'but', 'not', 'always', '.', 'california', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'ca', '.', 'to', 'really', 'complicate', 'things', ',', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'u.k.', ',', 'the', 'uk', ',', 'the', 'united', 'kingdom', ',', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'northern', 'ireland', 'becuase', 'sometimes', 'full', 'names', 'are', 'important', '.', 'further', ',', 'here', 'is', 'a', 'bunch']\n"
     ]
    }
   ],
   "source": [
    "corpus_lower = [word.lower() for word in corpus_words]\n",
    "print(corpus_lower[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is a sample corpus.', 'it haz some spelling errors and has numbers written two ways.', 'for example, it has both 1972 and ninety-six.', 'this sample corpus also uses abbreviations sometimes, but not always.', 'california is spelled out once but also written ca.']\n"
     ]
    }
   ],
   "source": [
    "#let's try and do the same thing for corpus sentences and see if this makes sense\n",
    "#or no\n",
    "\n",
    "corpus_sentences_lower = [word.lower() for word in corpus_sentences]\n",
    "print(corpus_sentences_lower[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doable, not sure how useful this is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spelling correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to correct some spelling. There are several spellchecking packages written for python so we're using one of those. And we're experimenting with a pip install in a JN which seems to work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting autocorrect\n",
      "  Downloading https://files.pythonhosted.org/packages/a1/83/9cecf8ea84b964b80205a081b808cc262160d6b1d6dc5c3dacd8c8e10b20/autocorrect-1.3.0.tar.gz (1.8MB)\n",
      "Building wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py): started\n",
      "  Building wheel for autocorrect (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\sonja\\AppData\\Local\\pip\\Cache\\wheels\\c6\\de\\f7\\d24f92af3335a698d38a54a43b8b40dcb3e8168a18a7f6f8c1\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-1.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 19.0.1, however version 20.2b1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "check = Speller(lang='en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we iterate over our corpus - we'll take lower, checking and correct each token where needed. Start with a new, empty list (I called mine 'corpus_correct_spell'). Working through the list the corrected word is appended to the empty list.\n",
    "Then, let's look at the first 100 words as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .....for word tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'corpus', '.', 'it', 'had', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', '.', 'for', 'example', ',', 'it', 'has', 'both', '1972', 'and', 'ninety-six', '.', 'this', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', ',', 'but', 'not', 'always', '.', 'california', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'ca', '.', 'to', 'really', 'complicate', 'things', ',', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'u.k.', ',', 'the', 'uk', ',', 'the', 'united', 'kingdom', ',', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'northern', 'ireland', 'because', 'sometimes', 'full', 'names', 'are', 'important', '.', 'further', ',', 'here', 'is', 'a', 'bunch']\n"
     ]
    }
   ],
   "source": [
    "corpus_correct_spell = []\n",
    "\n",
    "for word in corpus_lower:\n",
    "    corpus_correct_spell.append(check(word))    \n",
    "\n",
    "print(corpus_correct_spell[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#not bad - haz turned to had, not has. Nothing is 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### .... for sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is a sample corpus.', 'it had some spelling errors and has numbers written two ways.', 'for example, it has both 1972 and ninety-six.', 'this sample corpus also uses abbreviations sometimes, but not always.', 'california is spelled out once but also written ca.', 'to really complicate things, another country name is written as the u.k., the uk, the united kingdom, the united kingdom of great britain and the united kingdom of great britain and northern ireland because sometimes full names are important.', 'further, here is a bunch of unrelated text just to fill up the space.', 'this privacy policy (â\\x80\\x9cprivacy policyâ\\x80\\x9d) is intended to inform you of some policies and practices regarding the collection, use, and disclosure of your personal information through our site and any other sites that links to this privacy policy (the â\\x80\\x9csiteâ\\x80\\x9d).', 'we define â\\x80\\x9cpersonal informationâ\\x80\\x9d as information that allows someone to identify you personally or contact you, including for example your name, address, telephone number, and email address.', 'by registering with us or using our site, you expressly consent to the collection, use, processing, and disclosure of your personal information in accordance with this privacy policy.']\n"
     ]
    }
   ],
   "source": [
    "corpus_correct_sentence_spell = []\n",
    "\n",
    "for word in corpus_sentences_lower:\n",
    "    corpus_correct_sentence_spell.append(check(word))    \n",
    "\n",
    "print(corpus_correct_sentence_spell[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output comparable, just \"sliced up\" differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions (RegEx) to replace specific terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular Expressions is how seach & replace works in text docs. But RegEx is actually stronger than that because you can use it to identify combinations of letters, numbers, symbols, spaces and more, some of which can be repeated more than once or can be optional. Lots more to learn on RegEx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'corpus', '.', 'it', 'had', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', '.', 'for', 'example', ',', 'it', 'has', 'both', '1972', 'and', '96', '.', 'this', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', ',', 'but', 'not', 'always', '.', 'california', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'ca', '.', 'to', 'really', 'complicate', 'things', ',', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'u.k.', ',', 'the', 'uk', ',', 'the', 'united', 'kingdom', ',', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'northern', 'ireland', 'because', 'sometimes', 'full', 'names', 'are', 'important', '.', 'further', ',', 'here', 'is', 'a', 'bunch']\n"
     ]
    }
   ],
   "source": [
    "corpus_numbers = [re.sub(r\"ninety-six\", \"96\", word) for word in corpus_correct_spell]  # Defines a new variable create by substituting\n",
    "                                                    # '96' for 'ninety-six' in corpus_words\n",
    "\n",
    "print(corpus_numbers[:100])                                            # Prints the first 100 items in the newly created corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Super but only works on 96. So we need to find a multiple replace option.\n",
    "#Note: this function works on strings, so I applied it to 'corpus' our original \n",
    "#raw text. We can either put a step like this as the first step in a pipeline, \n",
    "#or we can adapt the code to iterate over a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(dict, text):\n",
    "  # Create a regular expression  from the dictionary keys\n",
    "  regex = re.compile(\"(%s)\" % \"|\".join(map(re.escape, dict.keys())))\n",
    "\n",
    "  # For each match, look-up corresponding value in dictionary\n",
    "  return regex.sub(lambda mo: dict[mo.string[mo.start():mo.end()]], text) \n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "\n",
    "  dict = {\n",
    "    \"CA\" : \"California\",\n",
    "    \"United Kingdom\" : \"U.K.\",\n",
    "    \"United Kingdom of Great Britain and Northern Ireland\" : \"U.K.\",\n",
    "    \"United Kingdom of Great Britain\" : \"U.K.\",\n",
    "    \"UK\" : \"U.K.\",\n",
    "    \"Privacy Policy\" : \"noodle soup\",\n",
    "  } \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a sample corpus. It haz some spelling errors and has numbers written two ways. For example, it has both 1972 and ninety-six. \n",
      "\n",
      "This sample corpus also uses abbreviations sometimes, but not always. California is spelled out once but also written California. \n",
      "\n",
      "To really complicate things, another country name is written as the U.K., the U.K., the U.K., the U.K. of Great Britain and The U.K. of Great Britain and Northern Ireland becuase sometimes full names are important. \n",
      "\n",
      "Further, here is a bunch of unrelated toxt just to fill up the space. \n",
      "\n",
      "This privacy policy (ânoodle soupâ) is intended to inform you of some policies and practices regarding the collection, use, and disclosure of your Personal Information through our site and any other sites that links to this noodle soup (the âSiteâ). We define âPersonal Informationâ as information that allows someone to identify you personally or contact you, including for example your name, address, telephone number, and email address.\n",
      "\n",
      "By registering with us or using our Site, you expressly consent to the collection, use, processing, and disclosure of your Personal Information in accordance with this noodle soup. If you reside outside the United States, you understand that your Personal Information may be processed in countries (including the United States) where laws regarding processing personal information may be less stringent than in your country.\n",
      "\n",
      "Capitalized terms used in this noodle soup but not defined in this noodle soup have the meanings given to them in the Terms of Use or Contributor Terms, as applicable.\n",
      "\n",
      "Personal Information You Provide to Us. When you register for an account on our Site, we collect your name, e-mail address, and e-mail preferences, and password for your account. You may also provide us additional information on an optional basis indicated as such in connection with your account and/or your use of our Site. When you choose to participate in a survey, we may collect your name, e-mail address, mailing address, phone number, age, gender and/or other requested information. When you give us Feedback, we may collect any information that is contained in your Feedback. When you contact us by sending us an e-mail, fax, or letter, we collect your e-mail address, fax number, or mailing address, and any information contained in the e-mail, fax, or letter you send us.\n",
      "\n",
      "Personal Information You Provide to Us Using Social Networking Connections. When you register your account with Facebook, Twitter, or other third party social networking services (each, a âSocial Networking Siteâ), we collect basic information about you and your friends that is provided to us by the Social Networking Sites. We combine this information with other personal information we collect from the Service. If you elect to share any information with Social Networking Sites while using our Site, we will share such information with the Social Networking Site and their use of such information will be subject to their own privacy policies and not by this noodle soup.\n",
      "\n",
      "Personal Information Collected via Technology. As you use our Site, some information may also be collected passively, including your Internet protocol address, browser type, the website you visited before browsing our Site, pages you viewed, how long you spent on a page, browser language, and access time.\n",
      "\n",
      "Cookies. We may automatically collect information using âcookies.â Cookies are small data files stored on your hard drive by a website. Among other things, cookies help us make our Site and your experience better. We use cookies to see which parts and features of our Site are popular and to count visits to our Site.\n",
      "\n",
      "Web Beacons. We may log information using digital images called âweb beaconsâ (also called âpixel tagsâ) on our Site or in our emails. We use web beacons to manage cookies, count visits, and to learn what marketing works and what does not. We also use web beacons to tell if you open or act on our emails.\n",
      "\n",
      "Analytics. We use Google Analytics and other analytics services to help analyze how users use the Site. These analytics services uses cookies to collect and store information such as how often users visit the Site, what pages they visit, and what other sites they used prior to coming to the Site. We use the information we get from these analytics services only to improve our Site and our services. Please see the following links for more information about Google Analytics: http://www.google.com/privacy.html and https://tools.google.com/dlpage/gaoptout/.\n",
      "Personal Information from Other Sources. We may receive Personal Information about you from other sources, including other users. We may associate this information with the other Personal Information we have collected about you.\n",
      "\n",
      "Use of Information\n",
      "We use your Personal Information to respond to requests that you make, to personalize your online experience on our Site, and to provide and improve our Site and its associated features and our services.\n",
      "\n",
      "We track IP protocol addresses in conjunction with session Cookies to analyze our web page flow and to keep track of your session information and clickstream data. We use persistent Cookies to keep track of your login name and password in connection with your use of, and uploading of Your Content through, our Site and to personalize your online experience on our Site.\n",
      "\n",
      "We may use your Personal Information to send you electronic newsletters or promotional e-mails, unless you have requested not to receive such promotional communications from us. We may also use your Personal Information to send you communications relating to any of our programs for which you have registered, and you may opt out of these communications if you terminate your account on our Site.\n",
      "\n",
      "We may also use your Personal Information to protect, investigate, and deter against fraudulent, unauthorized, or illegal activity.\n",
      "\n",
      "We may create Anonymous Information records from Personal Information. We reserve the right to use and disclose Anonymous Information at our discretion. âAnonymous Informationâ means information that is not associated with or linked to your Personal Information.This privacy policy (ânoodle soupâ) is intended to inform you of Riot New Media Group, Incâs (âRiot New Media Group, Inc,â âwe,â âus,â or âourâ) policies and practices regarding the collection, use, and disclosure of your Personal Information through our site located at https://bookriot.com/ and any other sites that links to this noodle soup (the âSiteâ). We define âPersonal Informationâ as information that allows someone to identify you personally or contact you, including for example your name, address, telephone number, and email address.\n",
      "\n",
      "By registering with us or using our Site, you expressly consent to the collection, use, processing, and disclosure of your Personal Information in accordance with this noodle soup. If you reside outside the United States, you understand that your Personal Information may be processed in countries (including the United States) where laws regarding processing personal information may be less stringent than in your country.\n",
      "\n",
      "Capitalized terms used in this noodle soup but not defined in this noodle soup have the meanings given to them in the Terms of Use or Contributor Terms, as applicable.\n",
      "\n",
      "Personal Information You Provide to Us. When you register for an account on our Site, we collect your name, e-mail address, and e-mail preferences, and password for your account. You may also provide us additional information on an optional basis indicated as such in connection with your account and/or your use of our Site. When you choose to participate in a survey, we may collect your name, e-mail address, mailing address, phone number, age, gender and/or other requested information. When you give us Feedback, we may collect any information that is contained in your Feedback. When you contact us by sending us an e-mail, fax, or letter, we collect your e-mail address, fax number, or mailing address, and any information contained in the e-mail, fax, or letter you send us.\n",
      "\n",
      "Personal Information You Provide to Us Using Social Networking Connections. When you register your account with Facebook, Twitter, or other third party social networking services (each, a âSocial Networking Siteâ), we collect basic information about you and your friends that is provided to us by the Social Networking Sites. We combine this information with other personal information we collect from the Service. If you elect to share any information with Social Networking Sites while using our Site, we will share such information with the Social Networking Site and their use of such information will be subject to their own privacy policies and not by this noodle soup.\n",
      "\n",
      "Personal Information Collected via Technology. As you use our Site, some information may also be collected passively, including your Internet protocol address, browser type, the website you visited before browsing our Site, pages you viewed, how long you spent on a page, browser language, and access time.\n"
     ]
    }
   ],
   "source": [
    "#Now apply the function\n",
    "#We have options about which variable to apply it to. I would like to  apply it\n",
    "#to the spell corrected, lower case, word tokenised corpus. However that gives\n",
    "#me an error. I think due to the fact that this is a list, rather than a string.\n",
    "#So I am going with the flow and applying it to a string - the raw corpus.\n",
    "\n",
    "corpus_replace = multiple_replace(dict, corpus)#_correct_spell)\n",
    "print(corpus_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### filter out irrelevancies..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's filter out punctuation. We can define a string that includes all the standard English language punctuation, and then use that to iterate over corpus_words, removing anything that matches.\n",
    "\n",
    "But wait... Do we really want to remove the hyphen in 'ninety-six' or words like 'lactose-free'? Or the apostrophe in contractions or possessives?\n",
    "\n",
    "There are no right or wrong answers here. Every project will have to decide, based on the research questions, what is the right choice for the specific context. \n",
    "\n",
    "Here we want to remove the full stops, even from 'u.k.' so that it becomes identical to 'uk'. Also we don't want to remove dashes or apostrophes. Those are punctuation marks that occur in the middle of words and do add meaning to the word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”\n",
      "...\n",
      "{33: None, 34: None, 35: None, 36: None, 37: None, 38: None, 40: None, 41: None, 42: None, 43: None, 44: None, 46: None, 47: None, 58: None, 59: None, 60: None, 61: None, 62: None, 63: None, 64: None, 91: None, 92: None, 93: None, 94: None, 95: None, 96: None, 123: None, 124: None, 125: None, 126: None, 8220: None, 8221: None}\n",
      "...\n",
      "['This', 'is', 'a', 'sample', 'corpus', '', 'It', 'haz', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', '', 'For', 'example', '', 'it', 'has', 'both', '1972', 'and', 'ninety-six', '', 'This', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', '', 'but', 'not', 'always', '', 'California', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'CA', '', 'To', 'really', 'complicate', 'things', '', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'UK', '', 'the', 'UK', '', 'the', 'United', 'Kingdom', '', 'the', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'The', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'Northern', 'Ireland', 'becuase', 'sometimes', 'full', 'names', 'are', 'important', '', 'Further', '', 'here', 'is', 'a', 'bunch']\n"
     ]
    }
   ],
   "source": [
    "English_punctuation = \"!\\\"#$%&()*+,./:;<=>?@[\\]^_`{|}~“”\"      # Define a variable with all the punctuation to remove.\n",
    "print(English_punctuation)                                     # Print that defined variable, just to check it is correct.\n",
    "print(\"...\")                                                   # Print an ellipsis, just to make the output more readable.\n",
    "\n",
    "table_punctuation = str.maketrans('','', English_punctuation)  # The python function 'maketrans' creates a table that maps\n",
    "print(table_punctuation)                                       # the punctation marks to 'None'. Print the table to check. \n",
    "print(\"...\")                                                   # Just to be clear, '!' is 33 in Unicode, and '\\' is 34, etc.\n",
    "                                                               # 'None' is python for nothing, not a string of the word \"none\".\n",
    "    \n",
    "corpus_no_punct = [w.translate(table_punctuation) for w in corpus_words]  \n",
    "                                                               # Iterate over corpus_words, turning punctuation to nothing.\n",
    "print(corpus_no_punct[:100])                                   # Print the 1st 100 items in corpus_no_punct to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'corpus', 'It', 'haz', 'some', 'spelling', 'errors', 'and', 'has', 'numbers', 'written', 'two', 'ways', 'For', 'example', 'it', 'has', 'both', '1972', 'and', 'ninety-six', 'This', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', 'but', 'not', 'always', 'California', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'CA', 'To', 'really', 'complicate', 'things', 'another', 'country', 'name', 'is', 'written', 'as', 'the', 'UK', 'the', 'UK', 'the', 'United', 'Kingdom', 'the', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'The', 'United', 'Kingdom', 'of', 'Great', 'Britain', 'and', 'Northern', 'Ireland', 'becuase', 'sometimes', 'full', 'names', 'are', 'important', 'Further', 'here', 'is', 'a', 'bunch', 'of', 'unrelated', 'toxt', 'just', 'to', 'fill', 'up', 'the', 'space', 'This', 'privacy', 'policy', 'â\\x80\\x9cPrivacy']\n"
     ]
    }
   ],
   "source": [
    "corpus_no_space = list(filter(None, corpus_no_punct))     # This filters out the empty string from the no_punct list.\n",
    "\n",
    "print(corpus_no_space[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hmm more to learn here on regex and applying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the tutorial:\n",
    "     - Stopwords are typically conjunctions ('and', 'or'), prepositions ('to', 'around'), determiners ('the', 'an'), possessives ('s) and the like. The are REALLY common in all languages, and tend to occur at about the same ratio in all kinds of writing, regardless of who did the writing or what it is about. These words are definitely important for structure as they make all the difference between \"Freeze or I'll shoot!\" and \"Freeze and I'll shoot!\".\n",
    "\n",
    "Buuuut... Many for many text-mining analyses, especially those that take the bag of words approach, these words don't have a whole lot of meaning in and of themselves. Thus, we want to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Download basic stop words function\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it's\", 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she's\", 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', 'were', 'weren', \"weren't\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']\n"
     ]
    }
   ],
   "source": [
    "#Let's see what they look like\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(sorted(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'corpus', '.', 'spelling', 'errors', 'numbers', 'written', 'two', 'ways', '.', 'example', ',', '1972', 'ninety-six', '.', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', ',', 'always', '.', 'california', 'spelled', 'also', 'written', 'ca', '.', 'really', 'complicate', 'things', ',', 'another', 'country', 'name', 'written', 'u.k.', ',', 'uk', ',', 'united', 'kingdom', ',', 'united', 'kingdom', 'great', 'britain', 'united', 'kingdom', 'great', 'britain', 'northern', 'ireland', 'sometimes', 'full', 'names', 'important', '.', ',', 'bunch', 'unrelated', 'text', 'fill', 'space', '.', 'privacy', 'policy', '(', 'â\\x80\\x9cprivacy', 'policyâ\\x80\\x9d', ')', 'intended', 'inform', 'policies', 'practices', 'regarding', 'collection', ',', 'use', ',', 'disclosure', 'personal', 'information', 'site', 'sites', 'links', 'privacy', 'policy', '(', 'â\\x80\\x9csiteâ\\x80\\x9d', ')', '.', 'define', 'â\\x80\\x9cpersonal', 'informationâ\\x80\\x9d', 'information', 'allows', 'someone']\n"
     ]
    }
   ],
   "source": [
    "#Now let's remove those stop_words\n",
    "#create another list called corpus_no_stop_words. \n",
    "#iterate over corpus_correct_spell, looking at them one by one and appending them\n",
    "#to corpus_no_stop_words if and only if they do not match any of the items in the \n",
    "#stop_words list.\n",
    "\n",
    "corpus_no_stop_words = []\n",
    "\n",
    "for word in corpus_correct_spell:\n",
    "    if word not in stop_words:\n",
    "        corpus_no_stop_words.append(word)\n",
    "        \n",
    "        \n",
    "print(corpus_no_stop_words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'corpus', '.', 'It', 'haz', 'spelling', 'errors', 'numbers', 'written', 'two', 'ways', '.', 'For', 'example', ',', '1972', 'ninety-six', '.', 'This', 'sample', 'corpus', 'also', 'uses', 'abbreviations', 'sometimes', ',', 'always', '.', 'California', 'spelled', 'also', 'written', 'CA', '.', 'To', 'really', 'complicate', 'things', ',', 'another', 'country', 'name', 'written', 'U.K.', ',', 'UK', ',', 'United', 'Kingdom', ',', 'United', 'Kingdom', 'Great', 'Britain', 'The', 'United', 'Kingdom', 'Great', 'Britain', 'Northern', 'Ireland', 'becuase', 'sometimes', 'full', 'names', 'important', '.', 'Further', ',', 'bunch', 'unrelated', 'toxt', 'fill', 'space', '.', 'This', 'privacy', 'policy', '(', 'â\\x80\\x9cPrivacy', 'Policyâ\\x80\\x9d', ')', 'intended', 'inform', 'policies', 'practices', 'regarding', 'collection', ',', 'use', ',', 'disclosure', 'Personal', 'Information', 'site', 'sites', 'links', 'Privacy', 'Policy']\n"
     ]
    }
   ],
   "source": [
    "#Let's try this on corpus words\n",
    "corpus_no_stop_words1 = []\n",
    "\n",
    "for word in corpus_words:\n",
    "    if word not in stop_words:\n",
    "        corpus_no_stop_words1.append(word)\n",
    "        \n",
    "        \n",
    "print(corpus_no_stop_words1[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Doesn't work so well - as stopwords may be lower or upper case and this is case\n",
    "#sensitive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consolidation - stemming\n",
    "\n",
    "\n",
    "\n",
    "We see that 'sample' has become 'sampl', which collapses 'sampled' together with 'samples' and 'sampling' and 'sample'. This puts plurals and verb tenses all in the same form so they can be counted as instances of the \"same\" word.\n",
    "\n",
    "We could run with this or decided to do more cleaning. If we want nouns and verbs seperate we need lemmatisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sampl', 'corpu', '.', 'spell', 'error', 'number', 'written', 'two', 'way', '.', 'exampl', ',', '1972', 'ninety-six', '.', 'sampl', 'corpu', 'also', 'use', 'abbrevi', 'sometim', ',', 'alway', '.', 'california', 'spell', 'also', 'written', 'ca', '.', 'realli', 'complic', 'thing', ',', 'anoth', 'countri', 'name', 'written', 'u.k.', ',', 'uk', ',', 'unit', 'kingdom', ',', 'unit', 'kingdom', 'great', 'britain', 'unit', 'kingdom', 'great', 'britain', 'northern', 'ireland', 'sometim', 'full', 'name', 'import', '.', ',', 'bunch', 'unrel', 'text', 'fill', 'space', '.', 'privaci', 'polici', '(', 'â\\x80\\x9cprivaci', 'policyâ\\x80\\x9d', ')', 'intend', 'inform', 'polici', 'practic', 'regard', 'collect', ',', 'use', ',', 'disclosur', 'person', 'inform', 'site', 'site', 'link', 'privaci', 'polici', '(', 'â\\x80\\x9csiteâ\\x80\\x9d', ')', '.', 'defin', 'â\\x80\\x9cperson', 'informationâ\\x80\\x9d', 'inform', 'allow', 'someon']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "corpus_stemmed = [porter.stem(word) for word in corpus_no_stop_words]\n",
    "print(corpus_stemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation\n",
    "The results show that our examples produce good output - 'rocks', 'corpora' and 'cares' are all de-pluralised correctly. The examples with part of speech tags also show that 'caring' and 'cared' are both correctly converted to 'care' as the base verb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sonja\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "rocks : rock\n",
      "corpora : corpus\n",
      "cares : care\n",
      "caring : care\n",
      "cared : care\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer() \n",
    " \n",
    "print('rocks :', lemmatizer.lemmatize('rocks'))              #a few examples of lemmatising as a de-pluraliser\n",
    "print('corpora :', lemmatizer.lemmatize('corpora'))\n",
    "print('cares :', lemmatizer.lemmatize('cares'))              #no part of speech tag supplied, so 'cares' is treated as noun\n",
    "print('caring :', lemmatizer.lemmatize('caring', pos = \"v\")) #when part of speech tag added, 'caring' is treated as verb             \n",
    "print('cared :', lemmatizer.lemmatize('cared', pos = \"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'sample', 'corpus', '.', 'it', 'had', 'some', 'spelling', 'error', 'and', 'ha', 'number', 'written', 'two', 'way', '.', 'for', 'example', ',', 'it', 'ha', 'both', '1972', 'and', 'ninety-six', '.', 'this', 'sample', 'corpus', 'also', 'us', 'abbreviation', 'sometimes', ',', 'but', 'not', 'always', '.', 'california', 'is', 'spelled', 'out', 'once', 'but', 'also', 'written', 'ca', '.', 'to', 'really', 'complicate', 'thing', ',', 'another', 'country', 'name', 'is', 'written', 'a', 'the', 'u.k.', ',', 'the', 'uk', ',', 'the', 'united', 'kingdom', ',', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'the', 'united', 'kingdom', 'of', 'great', 'britain', 'and', 'northern', 'ireland', 'because', 'sometimes', 'full', 'name', 'are', 'important', '.', 'further', ',', 'here', 'is', 'a', 'bunch']\n"
     ]
    }
   ],
   "source": [
    "corpus_lemmed = [lemmatizer.lemmatize(word) for word in corpus_correct_spell]\n",
    "\n",
    "print(corpus_lemmed[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result are mixed.\n",
    "No part of speech tags in our corpus, so everything was treated as nouns. \n",
    "The corpus has been effectively de-pluralised, but all of the different verb tenses remain. \n",
    "Part of Speech tags - in next JN!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
